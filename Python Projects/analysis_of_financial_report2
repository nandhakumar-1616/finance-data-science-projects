import requests
from bs4 import BeautifulSoup
import json
import queue
import time
import os
from dotenv import load_dotenv
from requests.exceptions import ProxyError, HTTPError

load_dotenv()

BASE_URL = "https://www.walmart.com"
OUTPUT_FILE = "product_info.jsonl"

BASE_HEADERS = {
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36",
    "accept": "application/json",
    "accept-language": "en-US",
    "accept-encoding": "gzip, deflate, br, zstd",
}

host = 'brd.superproxy.io'
port = 22225
username = os.environ['BRD_USERNAME']
password = os.environ['BRD_PASSWORD']

proxy_url = f'http://{username}:{password}@{host}:{port}'

proxies = {
    'http': proxy_url,
    'https': proxy_url
}

search_queries = ["computers", "laptops", "desktops", "monitors", "printers", "hard+drives", "usb", "cords", "cameras", 
                  "mouse", "keyboard", "microphones", "speakers", "radio", "tablets", "android", "apple", "watch", "smart+watch", 
                  "fridge", "airconditioning", "wifi", "router", "modem", "desk", "xbox", "playstation", "nintendo"]

product_queue = queue.Queue()
seen_urls = set()

def get_product_links_from_search_page(query, page_number):
    search_url = f"https://www.walmart.com/search?q={query}&page={page_number}"
    max_retries = 5
    backoff_factor = 3
    for attempt in range(max_retries):
        try:
            response = requests.get(search_url, headers=BASE_HEADERS, proxies=proxies)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            product_links = []

            found = False
            for a_tag in soup.find_all('a', href=True):
                if '/ip/' in a_tag['href']:
                    found = True
                    if "https" in a_tag['href']:
                        full_url = a_tag['href']
                    else:
                        full_url = BASE_URL + a_tag['href']

                    if full_url not in seen_urls:
                        product_links.append(full_url)

            if not found:
                print("\n\n\nSOUP WHEN NOT FOUND", soup)

            return product_links
        
        except ProxyError as e:
            wait_time = backoff_factor ** attempt
            print(f"Proxy error: {e}. Retrying in {wait_time} seconds...")
            time.sleep(wait_time)

        except HTTPError as e:
            if e.response.status_code == 412:
                print(f"Precondition Failed (412): {e}. Skipping URL.")
                break
            wait_time = backoff_factor ** attempt
            print(f"HTTP error: {e}. Retrying in {wait_time} seconds...")
            time.sleep(wait_time)

        except Exception as e:
            print(f"Failed to get product links for query: {query} on page: {page_number}. Error: {e}")
            break

    print(f"Skipping query after {max_retries} retries: {query} on page: {page_number}")
    return []

def get_product_details(product_url):
    #start 2/2
    max_retries = 5
    backoff_factor = 3
    for attempt in range(max_retries):
        try:
            response = requests.get(product_url, headers=BASE_HEADERS, proxies=proxies)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            title = soup.find('h1', class_='prod-ProductTitle').get_text(strip=True) if soup.find('h1', class_='prod-ProductTitle') else 'N/A'
            price = soup.find('span', class_='price-characteristic').get_text(strip=True) if soup.find('span', class_='price-characteristic') else 'N/A'
            rating = soup.find('span', class_='ReviewsHeader-ratingPrefix').get_text(strip=True) if soup.find('span', class_='ReviewsHeader-ratingPrefix') else 'N/A'
            num_reviews = soup.find('span', class_='ReviewsHeader-reviewCount').get_text(strip=True) if soup.find('span', class_='ReviewsHeader-reviewCount') else 'N/A'

            product_info = {
                "url": product_url,
                "title": title,
                "price": price,
                "rating": rating,
                "num_reviews": num_reviews
            }

            return product_info
        
        except ProxyError as e:
            wait_time = backoff_factor ** attempt
            print(f"Proxy error: {e}. Retrying in {wait_time} seconds...")
            time.sleep(wait_time)

        except HTTPError as e:
            if e.response.status_code == 412:
                print(f"Precondition Failed (412): {e}. Skipping URL.")
                break
            wait_time = backoff_factor ** attempt
            print(f"HTTP error: {e}. Retrying in {wait_time} seconds...")
            time.sleep(wait_time)

        except Exception as e:
            print(f"Failed to get product details for URL: {product_url}. Error: {e}")
            break

    print(f"Skipping product after {max_retries} retries: {product_url}")
    return None

